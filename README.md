# play-with-LLM-LoRA
a personal note about using LLM LoRA for finetuning

LoRA finetuning is efficient for large-scale LLM, however, there are many hiccups or bumps in the road when playing with LoRA. Thus, I'd like to share some training scripts/problems I met and the solutions/LoRA finetuning advice here. If you find these notes helpful, plz leave a star!
