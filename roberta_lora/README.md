**RoBERTA Large Lora Test**

I started from RoBERTA finetune first because it is simply and I can verify the results quickly. The running results are on the [wandb project](https://wandb.ai/alpha_ai/lora_test_roberta/workspace?workspace=). The benchmark is a personal dataset which could not be shared at this stage. You can change to your own dataset instead.

If you are unfamiliar with parallel training, here is a short description generated by GPT4:
- **TP (Tensor Parallelism)**: Tensor parallelism involves splitting up the computations within a single layer across multiple devices. This is in contrast to model parallelism, where different layers of the model are assigned to different devices. Tensor parallelism is often used in conjunction with model parallelism to train very large models that cannot fit on a single device.

- **PP (Pipeline Parallelism)**: In pipeline parallelism, the model layers are divided among different devices, and each device processes a part of the input data independently. The outputs from one device are passed as inputs to the next device in the pipeline, similar to an assembly line. Pipeline parallelism allows for more efficient use of device memory and can help train very large models.

- **DP (Data Parallelism)**: Data parallelism involves splitting the input data across multiple devices, with each device running a copy of the model. Each device processes a different part of the input data and computes gradients independently. The gradients are then combined (averaged) across all devices to update the model parameters. This is one of the most common methods for distributed training of deep learning models.

All running cases in 2 * A100 80GB server:
- [x] Full finetune in a single GPU (no accelerate)



**Full finetune in a single GPU (no accelerate)**

Uncomment the 4th line in roberta.py to use a single GPU